{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Engineering Capstone Project"
   ],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Enviroment setup"
   ],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import uuid\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import monotonically_increasing_id as mono_id\n",
    "import configparser"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Read config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "INPUT_DATA = config['LOCAL']['INPUT_DATA']\n",
    "INPUT_DATA_VACCINES = config['LOCAL']['INPUT_DATA_VACCINES']\n",
    "OUTPUT_DATA = config['LOCAL']['OUTPUT_DATA']\n",
    "DATA_COLUMNS = config['COMMON']['DATA_COLUMNS']"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@udf(StringType())\n",
    "def null_id_to_uuid (id):\n",
    "    if id == None or id == \"null\":\n",
    "        return str(uuid.uuid4().hex)\n",
    "    return id"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def write_parquet(df, parquet_name):\n",
    "    parquet_path = OUTPUT_DATA + f'{parquet_name}.parquet'\n",
    "    df.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "    print(f'Writing {parquet_name} Table DONE.')\n"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def read_parquet(parquet_name):\n",
    "    parquet_path = OUTPUT_DATA + f'{parquet_name}.parquet'\n",
    "    return spark.read.parquet(parquet_path)"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def check_nulls(df, columns_list, expected_value):\n",
    "    df.createOrReplaceTempView(\"viewcheck\")\n",
    "    sql_check = f\"SELECT COUNT(*) FROM viewcheck WHERE 1 <> 1 {''.join([' OR ' + c + ' IS NULL ' for c in columns_list])}\"\n",
    "    \n",
    "    dfcheck = spark.sql(sql_check)\n",
    "    \n",
    "    value_check = dfcheck.collect()[0][0]\n",
    "    \n",
    "    return value_check == expected_value"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def check_has_content(df):\n",
    "    return df.count() > 0"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Spark session\n",
    "spark = SparkSession \\\n",
    "        .builder\\\n",
    "        .appName(\"Brazilian COVID-19 Immunization\")\\\n",
    "        .getOrCreate()"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "In this step, weâ€™ll:\n",
    "\n",
    "* Identify and gather the data we'll be using for our project (at least two sources and more than 1 million rows).\n",
    "* Explain what end use cases we'd like to prepare the data for (e.g., analytics table, app back-end, source-of-truth database, etc.)\n",
    "\n",
    "We choose the following datasets:\n",
    "* Brazilian Government' dataset [COVID-19 population immunization program](https://dados.gov.br/dataset/covid-19-vacinacao/resource/ef3bd0b8-b605-474b-9ae5-c97390c197a8?inner_span=True)\n",
    "    * Complete data: \"Dados completos\"\n",
    "    * Data per Brazilian States: \"Dados XX\", where XX is the State code\n",
    "\n",
    "For development we include just a subset of data in /data/vaccines.csv.\n",
    "\n",
    "When executing the pipeline on production, we should download the complete dataset, replacing the file vaccines.csv.\n",
    "\n",
    "**The dataset will be prepared for a data warehouse analysis table, available for public use (citizens, press, health institutions), for monitoring population immunization rates.**."
   ],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vaccines_df = spark.read.csv(INPUT_DATA_VACCINES, sep=';', header=True)\n",
    "\n",
    "vaccines_df.printSchema()"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Explore and Assess the Data\n",
    "In this step we need:\n",
    "* Explore the data to identify data quality issues, like missing values, duplicate data, etc.\n",
    "* Document steps necessary to clean the data"
   ],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Read the data dictionary from JSON and extract the valid columns\n",
    "col_names = pd.read_json(DATA_COLUMNS, typ='series')\n",
    "valid_columns = col_names.index\n",
    "valid_columns"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get the difference between the dataframe colums and the valid columns\n",
    "columns_todrop = list(set(vaccines_df.columns) - set(valid_columns))\n",
    "\n",
    "columns_todrop"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Remove unused columns from dataframe\n",
    "vaccines_df = vaccines_df.drop(*columns_todrop)\n",
    "vaccines_df.printSchema()"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Replace the null values\n",
    "vaccines_df = vaccines_df\\\n",
    "    .withColumn('paciente_id_null', null_id_to_uuid(vaccines_df.paciente_id))\\\n",
    "    .drop('paciente_id')\\\n",
    "    .withColumnRenamed('paciente_id_null', 'paciente_id')\n",
    "    \n",
    "vaccines_df = vaccines_df.fillna(\\\n",
    "    {\\\n",
    "        'vacina_categoria_codigo': 0, \\\n",
    "        'vacina_categoria_nome': 'N/A', \\\n",
    "        'vacina_grupoatendimento_nome': 'N/A', \\\n",
    "        'paciente_enumsexobiologico': 'N/A',\\\n",
    "        'paciente_endereco_nmmunicipio': 'N/A', \\\n",
    "        'paciente_endereco_nmpais': 'N/A', \\\n",
    "        'paciente_endereco_uf': 'N/A', \\\n",
    "        'estalecimento_nofantasia': 'N/A'\n",
    "    })"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vaccines_df.printSchema()"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "write_parquet(vaccines_df, 'staging_immunization')\n",
    "\n",
    "vaccines_df = read_parquet('staging_immunization')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Define the Data Model\n",
    "_Map out the conceptual data model and explain why you chose that model_\n",
    "\n",
    "The data model is a star schema consisting of 5 Dimensions table and 1 Fact table:\n",
    "  * Dimensions tables:\n",
    "      * vaccines table: Vaccines and suppliers\n",
    "      * health_institution table: Hospitals, Nursing home, Clinics \n",
    "      * category table: Priority groups\n",
    "      * population_group table: Demograph group (professions, age group, ethnicity)\n",
    "      * patient table: Demograph data (age, city, gender)\n",
    "  * Fact table:\n",
    "      * immunization table: Dimensions, First | second dose, date\n",
    "\n",
    "![ER Data Model - Star Scheme](./docs/er-model-star.jpg)\n",
    "\n",
    "_List the steps necessary to pipeline the data into the chosen data model_\n",
    "* ETL starts the enviroment setup: imports, read config file, def functions and create Spark Session\n",
    "* ETL script takes source data (Brazilian Government' dataset COVID-19 population immunization program)\n",
    "* Raw data is read into dataframe and cleaned (remove unused columns, fill nulls) \n",
    "* For each dimension and fact table \n",
    "\t* Create a temporary view table\n",
    "\t* Read data to new dataframe\n",
    "    * Check data quality: key columns don't have nulls, each table has content\n",
    "\t* Create id/indexes (if necessary)\n",
    "\t* Write parquet files"
   ],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Run ETL to Model the Data\n",
    "* Create the data pipelines and the data model\n",
    "* Include a data dictionary\n",
    "* Run data quality checks to ensure the pipeline ran as expected\n",
    "\t* Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    "\t* Unit tests for the scripts to ensure they are doing the right thing\n",
    "\t* Source/count checks to ensure completeness"
   ],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create vaccines table\n",
    "vaccines_df.createOrReplaceTempView(\"vaccines_table_DF\")\n",
    "vaccines_table_DF = spark.sql(\"\"\"\n",
    "    SELECT  DISTINCT vacina_codigo AS id, \n",
    "                     vacina_nome AS name, \n",
    "                     vacina_fabricante_nome AS supplier\n",
    "    FROM vaccines_table_DF\n",
    "    ORDER BY supplier\n",
    "\"\"\")\n",
    "\n",
    "vaccines_table_DF.printSchema()\n",
    "vaccines_table_DF.show()"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check data quality\n",
    "if not check_nulls(vaccines_table_DF, ['id', 'name', 'supplier'], 0): raise Exception('Null: Vaccines tables')\n",
    "if not check_has_content(vaccines_table_DF): raise Exception('No content: Vaccines table')"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Write parquet file\n",
    "write_parquet(vaccines_table_DF, 'vaccines')"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create Health Institution table\n",
    "vaccines_df.createOrReplaceTempView(\"health_institution_table_DF\")\n",
    "health_institution_table_DF = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT estalecimento_nofantasia AS name,\n",
    "                    estabelecimento_razaosocial AS organization,\n",
    "                    estabelecimento_uf AS state,\n",
    "                    estabelecimento_municipio_nome AS city\n",
    "    FROM health_institution_table_DF\n",
    "    ORDER BY name\n",
    "\"\"\")\n",
    "\n",
    "health_institution_table_DF = health_institution_table_DF.select(mono_id().alias('id'), '*')\n",
    "health_institution_table_DF.printSchema()\n",
    "health_institution_table_DF.show()"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check data quality\n",
    "if not check_nulls(health_institution_table_DF, ['id', 'name', 'organization', 'state', 'city'], 0): raise Exception('Null: Health institution table')\n",
    "if not check_has_content(health_institution_table_DF): raise Exception('No content: Health institution table')"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Write parquet file\n",
    "write_parquet(health_institution_table_DF, 'health_institution')"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create Category table\n",
    "vaccines_df.createOrReplaceTempView(\"category_table_DF\")\n",
    "category_table_DF = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT vacina_categoria_codigo AS id,\n",
    "                    vacina_categoria_nome AS name\n",
    "            FROM category_table_DF\n",
    "            ORDER BY name\n",
    "\"\"\")\n",
    "\n",
    "category_table_DF.printSchema()\n",
    "category_table_DF.show()"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check data quality\n",
    "if not check_nulls(category_table_DF, ['id', 'name'], 0): raise Exception('Null: Category table')\n",
    "if not check_has_content(category_table_DF): raise Exception('No content: Category table')"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Write parquet file\n",
    "write_parquet(category_table_DF, 'category')"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create Population Groups table\n",
    "vaccines_df.createOrReplaceTempView(\"population_group_table_DF\")\n",
    "population_group_table_DF = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT vacina_grupoatendimento_codigo AS id,\n",
    "                    vacina_grupoatendimento_nome AS name\n",
    "            FROM population_group_table_DF\n",
    "        ORDER BY name\n",
    "\"\"\")\n",
    "\n",
    "population_group_table_DF.printSchema()\n",
    "population_group_table_DF.show()"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check data quality\n",
    "if not check_nulls(population_group_table_DF, ['id', 'name'], 0): raise Exception('Null: Population group table')\n",
    "if not check_has_content(population_group_table_DF): raise Exception('No content: Population group table')"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Write parquet file\n",
    "write_parquet(population_group_table_DF, 'population_group')"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create Patient table\n",
    "vaccines_df.createOrReplaceTempView(\"patient_table_DF\")\n",
    "patient_table_DF = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT paciente_id AS id,\n",
    "                    paciente_idade AS age,\n",
    "                    paciente_datanascimento AS birthdate,\n",
    "                    paciente_enumsexobiologico AS gender,\n",
    "                    paciente_endereco_nmpais AS country,\n",
    "                    paciente_endereco_uf AS state,\n",
    "                    paciente_endereco_nmmunicipio AS city\n",
    "            FROM patient_table_DF\n",
    "            ORDER BY id\n",
    "\"\"\")\n",
    "\n",
    "patient_table_DF.printSchema()\n",
    "patient_table_DF.show()"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1 = spark.sql(\"select * from patient_table_DF WHERE paciente_id is null\")\n",
    "\n",
    "df1.toPandas().style"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check data quality\n",
    "if not check_nulls(patient_table_DF, ['id', 'age', 'birthdate', 'gender', 'country', 'state', 'city'], 0): raise Exception('Null: Patient table')\n",
    "if not check_has_content(patient_table_DF): raise Exception('No content: Patient table')"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Write parquet file and get back to Spark:\n",
    "write_parquet(patient_table_DF, 'patient')"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get back Health Institution table from parquet\n",
    "health_institution_table_DF = read_parquet('health_institution')"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create Patient table and write parquet files\n",
    "vaccines_df.createOrReplaceTempView(\"vdf\")\n",
    "health_institution_table_DF.createOrReplaceTempView(\"hidf\")\n",
    "immunization_table_DF = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT vdf.paciente_id AS patient_id,\n",
    "            hidf.id AS health_institution_id,\n",
    "            vdf.vacina_categoria_codigo AS category_id,\n",
    "            vdf.vacina_grupoatendimento_codigo AS population_group_id,\n",
    "            vdf.vacina_codigo AS vaccines_id,\n",
    "            vdf.vacina_descricao_dose AS vaccines_dose,\n",
    "            vdf.vacina_dataaplicacao AS jab_date\n",
    "        FROM vdf INNER JOIN hidf\n",
    "        ON vdf.estalecimento_nofantasia == hidf.name\n",
    "        AND vdf.estabelecimento_razaosocial == hidf.organization\n",
    "        ORDER BY jab_date\n",
    "\"\"\")\n",
    "\n",
    "immunization_table_DF.printSchema()"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check data quality\n",
    "if not check_nulls(immunization_table_DF, \\\n",
    "        ['patient_id', \\\n",
    "        'health_institution_id', \\\n",
    "        'category_id', \\\n",
    "        'population_group_id', \\\n",
    "        'vaccines_id', \\\n",
    "        'vaccines_dose', \\\n",
    "        'jab_date'], 0): raise Exception('Null: Immunization table')\n",
    "if not check_has_content(immunization_table_DF): raise Exception('No content: Immunization table')"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Write parquet file and get back to Spark:\n",
    "write_parquet(immunization_table_DF, 'immunization')\n",
    "immunization_table_DF = read_parquet('immunization')"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "immunization_table_DF.show()"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: Complete Project Write Up\n",
    "Tools:\n",
    "* Python\n",
    "* Pandas\n",
    "* Spark\n",
    "\n",
    "These tools/technologies are apropriated to manipulate large dataset, processing in paralellized clusters\n",
    "\n",
    "**ETL script should be run weekly basis**, or whenever the .gov.br update the datasets\n",
    "\n",
    "How the script would approach the problem differently under the following scenarios:\n",
    "* If the data was increased by 100x: _Use Spark Clustered to parallel the data load_\n",
    "* If the pipelines were run on a daily basis by 7am: _Refactor the script to process only new informations. The [.gov.br API](https://dados.gov.br/dataset/covid-19-vacinacao/resource/97a8fbcf-941f-4d2e-91ba-dd467d5bdeac?inner_span=True) could be used to request the delta information_\n",
    "* If the database needed to be accessed by 100+ people: _Store the parquet files on a cloud data lake_"
   ],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sample queries"
   ],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vaccines_table_DF = read_parquet('vaccines')\n",
    "\n",
    "# Get Health institution by vaccines aplication\n",
    "vaccines_table_DF.createOrReplaceTempView(\"vdf\")\n",
    "health_institution_table_DF.createOrReplaceTempView(\"hidf\")\n",
    "immunization_table_DF.createOrReplaceTempView(\"idf\")\n",
    "\n",
    "health_institution_rank = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CASE\n",
    "            WHEN (GROUPING(hidf.name) = 1) THEN '# Total Institution'\n",
    "            ELSE hidf.name\n",
    "        END AS Institution,\n",
    "        CASE\n",
    "            WHEN (GROUPING(vdf.name) = 1) THEN '# Total Vaccines'\n",
    "            ELSE vdf.name\n",
    "        END AS Vaccine,\n",
    "        COUNT(*) AS TOTAL\n",
    "    FROM idf \n",
    "    INNER JOIN hidf ON idf.health_institution_id = hidf.id\n",
    "    INNER JOIN vdf ON idf.vaccines_id = vdf.id\n",
    "    GROUP BY CUBE(hidf.name, vdf.name)\n",
    "    ORDER BY Institution, TOTAL DESC\n",
    "\"\"\").toPandas()\n",
    "\n",
    "health_institution_rank.style"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get vaccines aplication\n",
    "vaccines_rank = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CASE\n",
    "            WHEN (GROUPING(vdf.supplier) = 1) THEN '# Total Supplier'\n",
    "            ELSE vdf.supplier\n",
    "        END AS Supplier,\n",
    "        CASE\n",
    "            WHEN (GROUPING(vdf.name) = 1) THEN '# Total Vaccines'\n",
    "            ELSE vdf.name\n",
    "        END AS Vaccine,\n",
    "        COUNT(*) AS TOTAL\n",
    "    FROM idf \n",
    "    INNER JOIN vdf ON idf.vaccines_id = vdf.id\n",
    "    GROUP BY CUBE(vdf.supplier, vdf.name)\n",
    "    ORDER BY 1, TOTAL DESC\n",
    "\"\"\").toPandas()\n",
    "\n",
    "vaccines_rank.style"
   ],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "editable": true
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f08008ae04b0180d9de48a4e6c46fd1dc6f0039899f9ca67c2a7ee0fcf33e7"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}