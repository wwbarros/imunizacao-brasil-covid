{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Progama brasileiro de imunização - COVID 19"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Objetivos\r\n",
    "\r\n",
    "Transformar os dados brutos dos registro de vacinação para um modelo analítico, disponibilizando para consulta pública.\r\n",
    "\r\n",
    "Utilizaremos os seguintes conjuntos de dados:\r\n",
    "* [Registros de Vacinação COVID19 do Ministério da Saúde](https://dados.gov.br/dataset/covid-19-vacinacao/resource/ef3bd0b8-b605-474b-9ae5-c97390c197a8?inner_span=True)\r\n",
    "    * O processamento será feito por Unidade Federativa, para fins de otimização\r\n",
    "\r\n",
    "**Os dados serão extraídos dos arquivos CSV fornecidos pelo Ministério da Saúde, e carregados em um conjunto de tabelas analíticas, seguindo o modelo [Star Schema](https://en.wikipedia.org/wiki/Star_schema).**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Enviroment setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Bibliotecas utilizadas\r\n",
    "import configparser\r\n",
    "import pandas as pd\r\n",
    "from pyspark.sql import SparkSession\r\n",
    "import pyspark.sql.types as T\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Read config file\r\n",
    "config = configparser.ConfigParser()\r\n",
    "config.read_file(open('dl.cfg'))\r\n",
    "\r\n",
    "DATA_COLUMNS = config['COMMON']['DATA_COLUMNS']\r\n",
    "DATA_LOCATION = config['COMMON']['DATA_LOCATION']\r\n",
    "INPUT_DATA = config[DATA_LOCATION]['INPUT_DATA']\r\n",
    "INPUT_DATA_VACCINES = config[DATA_LOCATION]['INPUT_DATA_VACCINES']\r\n",
    "OUTPUT_DATA = config[DATA_LOCATION]['OUTPUT_DATA']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def write_parquet(df, parquet_name):\r\n",
    "    parquet_path = OUTPUT_DATA + f'{parquet_name}.parquet'\r\n",
    "    df.write.mode(\"overwrite\").parquet(parquet_path)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def read_parquet(parquet_name):\r\n",
    "    parquet_path = OUTPUT_DATA + f'{parquet_name}.parquet'\r\n",
    "    return spark.read.parquet(parquet_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def check_nulls(df, columns_list, expected_value):\r\n",
    "    df.createOrReplaceTempView(\"viewcheck\")\r\n",
    "    sql_check = f\"SELECT COUNT(*) FROM viewcheck WHERE 1 <> 1 {''.join([' OR ' + c + ' IS NULL ' for c in columns_list])}\"\r\n",
    "    \r\n",
    "    dfcheck = spark.sql(sql_check)\r\n",
    "    \r\n",
    "    value_check = dfcheck.collect()[0][0]\r\n",
    "    \r\n",
    "    return value_check == expected_value"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def check_has_content(df):\r\n",
    "    return df.count() > 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def check_uniquekey(df, columns_list):\r\n",
    "    return df.groupBy(*columns_list).count().filter('count > 1').count() == 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Spark session\r\n",
    "spark = SparkSession \\\r\n",
    "        .builder\\\r\n",
    "        .appName(\"Brasil - Programa de imunização - COVID-19\")\\\r\n",
    "        .getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dados_imunizacao_schema = T.StructType([\\\r\n",
    "    T.StructField('document_id', T.StringType(), True),\\\r\n",
    "    T.StructField('paciente_id', T.StringType(), True),\\\r\n",
    "    T.StructField('paciente_idade', T.IntegerType(), True),\\\r\n",
    "    T.StructField('paciente_datanascimento', T.DateType(), True),\\\r\n",
    "    T.StructField('paciente_enumsexobiologico', T.StringType(), True),\\\r\n",
    "    T.StructField('paciente_racacor_codigo', T.StringType(), True),\\\r\n",
    "    T.StructField('paciente_racacor_valor', T.StringType(), True),\\\r\n",
    "    T.StructField('paciente_endereco_coibgemunicipio', T.StringType(), True),\\\r\n",
    "    T.StructField('paciente_endereco_copais', T.StringType(), True),\\\r\n",
    "    T.StructField('paciente_endereco_nmmunicipio', T.StringType(), True),\\\r\n",
    "    T.StructField('paciente_endereco_nmpais', T.StringType(), True),\\\r\n",
    "    T.StructField('paciente_endereco_uf', T.StringType(), True),\\\r\n",
    "    T.StructField('paciente_endereco_cep', T.StringType(), True),\\\r\n",
    "    T.StructField('paciente_nacionalidade_enumnacionalidade', T.StringType(), True),\\\r\n",
    "    T.StructField('estabelecimento_valor', T.IntegerType(), True),\\\r\n",
    "    T.StructField('estabelecimento_razaosocial', T.StringType(), True),\\\r\n",
    "    T.StructField('estalecimento_nofantasia', T.StringType(), True),\\\r\n",
    "    T.StructField('estabelecimento_municipio_codigo', T.StringType(), True),\\\r\n",
    "    T.StructField('estabelecimento_municipio_nome', T.StringType(), True),\\\r\n",
    "    T.StructField('estabelecimento_uf', T.StringType(), True),\\\r\n",
    "    T.StructField('vacina_grupoatendimento_codigo', T.IntegerType(), True),\\\r\n",
    "    T.StructField('vacina_grupoatendimento_nome', T.StringType(), True),\\\r\n",
    "    T.StructField('vacina_categoria_codigo', T.IntegerType(), True),\\\r\n",
    "    T.StructField('vacina_categoria_nome', T.StringType(), True),\\\r\n",
    "    T.StructField('vacina_lote', T.StringType(), True),\\\r\n",
    "    T.StructField('vacina_fabricante_nome', T.StringType(), True),\\\r\n",
    "    T.StructField('vacina_fabricante_referencia', T.StringType(), True),\\\r\n",
    "    T.StructField('vacina_dataaplicacao', T.DateType(), True),\\\r\n",
    "    T.StructField('vacina_descricao_dose', T.StringType(), True),\\\r\n",
    "    T.StructField('vacina_codigo', T.IntegerType(), True),\\\r\n",
    "    T.StructField('vacina_nome', T.StringType(), True),\\\r\n",
    "    T.StructField('sistema_origem', T.StringType(), True),\\\r\n",
    "    T.StructField('data_importacao_rnds', T.StringType(), True),\\\r\n",
    "    T.StructField('id_sistema_origem', T.StringType(), True)\\\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dados_imunizacao = spark.read.csv(INPUT_DATA_VACCINES, sep=';', header=True, schema=dados_imunizacao_schema)\r\n",
    "\r\n",
    "dados_imunizacao.printSchema()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Explore and Assess the Data\r\n",
    "In this step we need:\r\n",
    "* Explore the data to identify data quality issues, like missing values, duplicate data, etc.\r\n",
    "* Document steps necessary to clean the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Obter a lista de colunas que serão utilizadas\r\n",
    "col_names = pd.read_json(DATA_COLUMNS, typ='series')\r\n",
    "colunas_utilizadas = col_names.index\r\n",
    "colunas_utilizadas"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Obter as colunas não utilizadas\r\n",
    "colunas_naoutilizadas = list(set(dados_imunizacao.columns) - set(colunas_utilizadas))\r\n",
    "\r\n",
    "colunas_naoutilizadas"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Excluir do dataset as colunas não utilizadas\r\n",
    "dados_imunizacao = dados_imunizacao.drop(*colunas_naoutilizadas)\r\n",
    "dados_imunizacao.printSchema()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Trocar os valores Nulos\r\n",
    "dados_imunizacao = dados_imunizacao.fillna(\\\r\n",
    "    {\\\r\n",
    "        'vacina_categoria_codigo': 0, \\\r\n",
    "        'vacina_categoria_nome': 'N/A', \\\r\n",
    "        'vacina_grupoatendimento_nome': '',\\\r\n",
    "        'paciente_enumsexobiologico': 'N/A',\\\r\n",
    "        'paciente_endereco_nmmunicipio': 'N/A', \\\r\n",
    "        'paciente_endereco_nmpais': 'N/A', \\\r\n",
    "        'paciente_endereco_uf': 'N/A', \\\r\n",
    "        'estalecimento_nofantasia': 'N/A'\r\n",
    "    })"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dados_imunizacao.printSchema()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Cria o arquivo parquet com os dados temporários\r\n",
    "write_parquet(dados_imunizacao, 'dados_imunizacao')\r\n",
    "\r\n",
    "dados_imunizacao = read_parquet('dados_imunizacao')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dados_imunizacao.printSchema()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Define the Data Model\r\n",
    "_Map out the conceptual data model and explain why you chose that model_\r\n",
    "\r\n",
    "The data model is a star schema consisting of 5 Dimensions table and 1 Fact table:\r\n",
    "  * Dimensions tables:\r\n",
    "      * vaccines table: Vaccines and suppliers\r\n",
    "      * health_institution table: Hospitals, Nursing home, Clinics \r\n",
    "      * category table: Priority groups\r\n",
    "      * population_group table: Demograph group (professions, age group, ethnicity)\r\n",
    "      * patient table: Demograph data (age, city, gender)\r\n",
    "  * Fact table:\r\n",
    "      * immunization table: Dimensions, First | second dose, date\r\n",
    "\r\n",
    "![ER Data Model - Star Scheme](./docs/er-model-star.jpg)\r\n",
    "\r\n",
    "_List the steps necessary to pipeline the data into the chosen data model_\r\n",
    "* ETL starts the enviroment setup: imports, read config file, def functions and create Spark Session\r\n",
    "* ETL script takes source data (Brazilian Government' dataset COVID-19 population immunization program)\r\n",
    "* Raw data is read into dataframe and cleaned (remove unused columns, fill nulls) \r\n",
    "* For each dimension and fact table \r\n",
    "\t* Create a temporary view table\r\n",
    "\t* Read data to new dataframe\r\n",
    "    * Check data quality: key columns don't have nulls, each table has content\r\n",
    "\t* Create id/indexes (if necessary)\r\n",
    "\t* Write parquet files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Run ETL to Model the Data\r\n",
    "* Create the data pipelines and the data model\r\n",
    "* Include a data dictionary\r\n",
    "* Run data quality checks to ensure the pipeline ran as expected\r\n",
    "\t* Integrity constraints on the relational database (e.g., unique key, data type, etc.)\r\n",
    "\t* Unit tests for the scripts to ensure they are doing the right thing\r\n",
    "\t* Source/count checks to ensure completeness"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Criar tabela Vacinas\r\n",
    "dados_imunizacao.createOrReplaceTempView(\"vacinas_df\")\r\n",
    "vacinas_df = spark.sql(\"\"\"\r\n",
    "    SELECT  DISTINCT vacina_codigo AS codigo, \r\n",
    "                     vacina_nome AS descricao, \r\n",
    "                     vacina_fabricante_nome AS fornecedor\r\n",
    "    FROM vacinas_df\r\n",
    "    ORDER BY codigo, fornecedor, descricao\r\n",
    "\"\"\")\r\n",
    "\r\n",
    "vacinas_df.printSchema()\r\n",
    "vacinas_df.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# Verifica valores nulos ou se há valores, antes de salvar a tabela\r\n",
    "if not check_nulls(vacinas_df, ['codigo', 'descricao', 'fornecedor'], 0): raise Exception('Null: Vacinas')\r\n",
    "if not check_has_content(vacinas_df): raise Exception('No content: Vacinas')\r\n",
    "if not check_uniquekey(vacinas_df, ['codigo']): raise Exception('Unique Key Fail: Vacinas')\r\n",
    "write_parquet(vacinas_df, 'vacinas')"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "Exception",
     "evalue": "Unique Key Fail: Vacinas",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-258c36d1d796>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_nulls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvacinas_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'codigo'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'descricao'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fabricante'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Null: Vacinas'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_has_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvacinas_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No content: Vacinas'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_uniquekey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvacinas_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'codigo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unique Key Fail: Vacinas'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mwrite_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvacinas_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vacinas'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Unique Key Fail: Vacinas"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Criar tabela Estabelecimentos\r\n",
    "dados_imunizacao.createOrReplaceTempView(\"estabelecimento_df\")\r\n",
    "estabelecimento_df = spark.sql(\"\"\"\r\n",
    "    SELECT DISTINCT estabelecimento_valor AS codigo,\r\n",
    "                    estalecimento_nofantasia AS descricao,\r\n",
    "                    estabelecimento_razaosocial AS razaosocial,\r\n",
    "                    estabelecimento_uf AS uf,\r\n",
    "                    estabelecimento_municipio_nome AS municipio\r\n",
    "    FROM estabelecimento_df\r\n",
    "    ORDER BY uf, municipio, razaosocial, descricao\r\n",
    "\"\"\")\r\n",
    "\r\n",
    "estabelecimento_df.printSchema()\r\n",
    "estabelecimento_df.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Verifica valores nulos ou se há valores, antes de salvar a tabela\r\n",
    "if not check_nulls(estabelecimento_df, ['codigo', 'descricao', 'razaosocial', 'uf', 'municipio'], 0): raise Exception('Null: Estabelecimentos')\r\n",
    "if not check_has_content(estabelecimento_df): raise Exception('No content: Estabelecimentos')\r\n",
    "if not check_uniquekey(estabelecimento_df, ['codigo']): raise Exception('Unique Key Fail: Estabelecimentos')\r\n",
    "write_parquet(estabelecimento_df, 'estabelecimentos')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Criar tabela Categorias\r\n",
    "dados_imunizacao.createOrReplaceTempView(\"categorias_df\")\r\n",
    "categorias_df = spark.sql(\"\"\"\r\n",
    "    SELECT DISTINCT vacina_categoria_codigo AS codigo,\r\n",
    "                    vacina_categoria_nome AS descricao\r\n",
    "            FROM categorias_df\r\n",
    "            ORDER BY codigo\r\n",
    "\"\"\")\r\n",
    "\r\n",
    "categorias_df.printSchema()\r\n",
    "categorias_df.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Verifica valores nulos ou se há valores, antes de salvar a tabela\r\n",
    "if not check_nulls(categorias_df, ['codigo', 'descricao'], 0): raise Exception('Null: Categorias')\r\n",
    "if not check_has_content(categorias_df): raise Exception('No content: Categorias')\r\n",
    "if not check_uniquekey(categorias_df, ['codigo']): raise Exception('Unique Key Fail: Categorias')\r\n",
    "write_parquet(categorias_df, 'categorias')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Criar tabela Grupos de Atendimento\r\n",
    "dados_imunizacao.createOrReplaceTempView(\"grupos_atendimento_df\")\r\n",
    "grupos_atendimento_df = spark.sql(\"\"\"\r\n",
    "    SELECT DISTINCT vacina_grupoatendimento_codigo AS codigo,\r\n",
    "                    vacina_grupoatendimento_nome AS descricao\r\n",
    "            FROM grupos_atendimento_df\r\n",
    "        ORDER BY codigo\r\n",
    "\"\"\")\r\n",
    "\r\n",
    "grupos_atendimento_df.printSchema()\r\n",
    "grupos_atendimento_df.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Verifica valores nulos ou se há valores, antes de salvar a tabela\r\n",
    "if not check_nulls(grupos_atendimento_df, ['codigo', 'descricao'], 0): raise Exception('Null: Grupos de atendimento')\r\n",
    "if not check_has_content(grupos_atendimento_df): raise Exception('No content: Grupos de atendimento')\r\n",
    "if not check_uniquekey(grupos_atendimento_df, ['codigo']): raise Exception('Unique Key Fail: Grupos de atendimento')\r\n",
    "write_parquet(grupos_atendimento_df, 'grupos')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Criar tabela Pacientes\r\n",
    "dados_imunizacao.createOrReplaceTempView(\"pacientes_df\")\r\n",
    "pacientes_df = spark.sql(\"\"\"\r\n",
    "    SELECT DISTINCT paciente_id AS id,\r\n",
    "                    paciente_idade AS idade,\r\n",
    "                    paciente_datanascimento AS datanascimento,\r\n",
    "                    paciente_enumsexobiologico AS sexo,\r\n",
    "                    paciente_racacor_valor AS racacor,\r\n",
    "                    paciente_endereco_nmpais AS pais,\r\n",
    "                    paciente_endereco_uf AS uf,\r\n",
    "                    paciente_endereco_nmmunicipio AS municipio\r\n",
    "            FROM pacientes_df\r\n",
    "            WHERE paciente_id IS NOT NULL\r\n",
    "            ORDER BY id\r\n",
    "\"\"\")\r\n",
    "\r\n",
    "pacientes_df.printSchema()\r\n",
    "pacientes_df.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Verifica valores nulos ou se há valores, antes de salvar a tabela\r\n",
    "if not check_nulls(pacientes_df, ['id', 'idade', 'datanascimento', 'sexo', 'racacor', 'pais', 'uf', 'municipio'], 0): raise Exception('Null: Pacientes')\r\n",
    "if not check_has_content(pacientes_df): raise Exception('No content: Pacientes')\r\n",
    "if not check_uniquekey(pacientes_df, ['id']): raise Exception('Unique Key Fail: Pacientes')\r\n",
    "write_parquet(pacientes_df, 'pacientes')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create Patient table and write parquet files\r\n",
    "dados_imunizacao.createOrReplaceTempView(\"vacinacao\")\r\n",
    "vacinacao_df = spark.sql(\"\"\"\r\n",
    "    SELECT DISTINCT paciente_id AS paciente_id,\r\n",
    "            estabelecimento_valor AS estabelecimento,\r\n",
    "            vacina_categoria_codigo AS categoria,\r\n",
    "            vacina_grupoatendimento_codigo AS grupoatendimento,\r\n",
    "            vacina_codigo AS vacina,\r\n",
    "            vacina_lote AS lote,\r\n",
    "            vacina_descricao_dose AS dose,\r\n",
    "            vacina_dataaplicacao AS dataaplicacao\r\n",
    "        FROM vacinacao \r\n",
    "        WHERE paciente_id IS NOT NULL\r\n",
    "        ORDER BY dataaplicacao\r\n",
    "\"\"\")\r\n",
    "\r\n",
    "vacinacao_df.printSchema()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check data quality\r\n",
    "if not check_nulls(vacinacao_df, \\\r\n",
    "        ['paciente_id', \\\r\n",
    "        'estabelecimento', \\\r\n",
    "        'categoria', \\\r\n",
    "        'grupoatendimento', \\\r\n",
    "        'vacina', \\\r\n",
    "        'lote', \\\r\n",
    "        'dose', \\\r\n",
    "        'dataaplicacao'], 0): raise Exception('Null: Vacinacao')\r\n",
    "if not check_has_content(vacinacao_df): raise Exception('No content: Vacinacao')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Write parquet file and get back to Spark:\r\n",
    "write_parquet(vacinacao_df, 'vacinacao')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vacinacao_df.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data dictionary\r\n",
    "\r\n",
    "[Dict](./data-dictionary.json)\r\n",
    "```\r\n",
    "{\r\n",
    "  \"vaccines\": {\r\n",
    "    \"description\": \"Vaccines and suppliers\",\r\n",
    "    \"fields\": {\r\n",
    "      \"id\": \"Identity field\",\r\n",
    "      \"name\": \"Vaccine name\",\r\n",
    "      \"supplier\": \"Laboratory/supplier\"\r\n",
    "    }\r\n",
    "  },\r\n",
    "  \"health_institution\": {\r\n",
    "    \"description\": \"Hospitals, Nursing home, Clinics\",\r\n",
    "    \"fields\": {\r\n",
    "      \"id\": \"Identity field\",\r\n",
    "      \"name\": \"Institution name\",\r\n",
    "      \"organization\": \"\",\r\n",
    "      \"state\": \"Brazilian State\",\r\n",
    "      \"city\": \"City name\"\r\n",
    "    }\r\n",
    "  },\r\n",
    "  \"category\": {\r\n",
    "    \"description\": \"Priority groups\",\r\n",
    "    \"fields\": {\r\n",
    "      \"id\": \"Identity field\",\r\n",
    "      \"name\": \"Group name\"\r\n",
    "    }\r\n",
    "  },\r\n",
    "  \"population_group\": {\r\n",
    "    \"description\": \"Demograph group (professions, age group, ethnicity)\",\r\n",
    "    \"fields\": {\r\n",
    "      \"id\": \"Identity field\",\r\n",
    "      \"name\": \"Group name\"\r\n",
    "    }\r\n",
    "  },\r\n",
    "  \"patient\": {\r\n",
    "    \"description\": \"Demograph data (age, city, gender)\",\r\n",
    "    \"fields\": {\r\n",
    "      \"id\": \"Identity field\",\r\n",
    "      \"age\": \"Patient age (years)\",\r\n",
    "      \"birthdate\": \"Birth date\",\r\n",
    "      \"gender\": \"Male/Female\",\r\n",
    "      \"country\": \"Country (Brazil)\",\r\n",
    "      \"state\": \"Brazilian State\",\r\n",
    "      \"city\": \"City name\"\r\n",
    "    }\r\n",
    "  },\r\n",
    "  \"imunization\": {\r\n",
    "    \"description\": \"Dimensions, First | second dose, date\",\r\n",
    "    \"fields\": {\r\n",
    "      \"patient_id\": \"(FK) Patient Identity\",\r\n",
    "      \"health_institution_id\": \"(FK) Health institution Identity\",\r\n",
    "      \"category_id\": \"(FK) Category group Identity\",\r\n",
    "      \"population_group_id\": \"(FK) Population group Identity\",\r\n",
    "      \"vaccines_id\": \"(FK) Vaccine Identity\",\r\n",
    "      \"vaccines_dose\": \"First/Second dose\",\r\n",
    "      \"jab_date\": \"Vaccine date\"\r\n",
    "    }\r\n",
    "  }\r\n",
    "}\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: Complete Project Write Up\r\n",
    "Tools:\r\n",
    "* Python\r\n",
    "* Pandas\r\n",
    "* Spark\r\n",
    "\r\n",
    "These tools/technologies are apropriated to manipulate large dataset, processing in paralellized clusters\r\n",
    "\r\n",
    "**ETL script should be run weekly basis**, or whenever the .gov.br update the datasets\r\n",
    "\r\n",
    "How the script would approach the problem differently under the following scenarios:\r\n",
    "* If the data was increased by 100x: _Use Spark Clustered to parallel the data load_\r\n",
    "* If the pipelines were run on a daily basis by 7am: _Refactor the script to process only new informations. The [.gov.br API](https://dados.gov.br/dataset/covid-19-vacinacao/resource/97a8fbcf-941f-4d2e-91ba-dd467d5bdeac?inner_span=True) could be used to request the delta information_\r\n",
    "* If the database needed to be accessed by 100+ people: _Store the parquet files on a cloud data lake_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sample queries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vacinacao_df = read_parquet('vacinacao')\r\n",
    "estabelecimento_df = read_parquet('estabelecimentos')\r\n",
    "vacinas_df = read_parquet('vacinas')\r\n",
    "\r\n",
    "# Obter estabelecimentos por aplicação\r\n",
    "vacinas_df.createOrReplaceTempView(\"vdf\")\r\n",
    "estabelecimento_df.createOrReplaceTempView(\"hidf\")\r\n",
    "vacinacao_df.createOrReplaceTempView(\"idf\")\r\n",
    "\r\n",
    "estabelecimentos_total = spark.sql(\"\"\"\r\n",
    "    SELECT \r\n",
    "        CASE\r\n",
    "            WHEN (GROUPING(hidf.descricao) = 1) THEN '# Total Estabelecimento'\r\n",
    "            ELSE hidf.descricao\r\n",
    "        END AS Estabelecimento,\r\n",
    "        CASE\r\n",
    "            WHEN (GROUPING(vdf.descricao) = 1) THEN '# Total Vacina'\r\n",
    "            ELSE vdf.descricao\r\n",
    "        END AS Vacina,\r\n",
    "        COUNT(*) AS TOTAL\r\n",
    "    FROM idf \r\n",
    "    INNER JOIN hidf ON idf.estabelecimento = hidf.codigo\r\n",
    "    INNER JOIN vdf ON idf.vacina = vdf.codigo\r\n",
    "    GROUP BY CUBE(hidf.descricao, vdf.descricao)\r\n",
    "    ORDER BY Estabelecimento, TOTAL DESC, Vacina\r\n",
    "\"\"\").toPandas()\r\n",
    "\r\n",
    "estabelecimentos_total.head()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e3babd7ac79e0887df5a9e3ee8123f217b43f09743b73590444977beefbb8a7a"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}